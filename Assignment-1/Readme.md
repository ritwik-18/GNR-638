# ğŸ”¥ Custom Autograd + CNN Framework

## ğŸ“˜ COMPLETE IMPLEMENTATION README

This README is the **single implementation contract** for the entire project.
It specifies **exactly what must be written in every file**, what responsibilities each file has, and how all components connect.

The framework is built in strict layers:

```
Tensor Storage â†’ Autograd Engine â†’ Tensor Ops â†’ NN Layers â†’ Python Bindings â†’ Training Pipeline
```

âš ï¸ **Golden Rule:**
Never implement CNN layers before Tensor + Autograd + Basic Ops are working.

---

# ğŸ“ PROJECT STRUCTURE

```
cpp/
 â”œâ”€â”€ tensor.hpp
 â”œâ”€â”€ tensor.cpp
 â”œâ”€â”€ autograd.hpp
 â”œâ”€â”€ autograd.cpp
 â”œâ”€â”€ ops.hpp
 â”œâ”€â”€ ops.cpp
 â”œâ”€â”€ nn.hpp
 â”œâ”€â”€ nn.cpp
 â””â”€â”€ bindings.cpp

python/
 â”œâ”€â”€ framework.py
 â”œâ”€â”€ dataset.py
 â”œâ”€â”€ model.py
 â”œâ”€â”€ train.py
 â””â”€â”€ evaluate.py
```

---

# ğŸ§± C++ IMPLEMENTATION

---

# âœ… tensor.hpp

## Purpose

Defines the **Tensor container**.
Only storage + interface declarations live here.

## MUST DEFINE

### Class

```
class Tensor
```

### Data Members

```
vector<float> data;
vector<float> grad;
vector<int> shape;

bool requires_grad;

vector<Tensor*> parents;
function<void()> backward_fn;
```

### Method Declarations

```
Tensor(vector<int> shape, bool requires_grad=false);
int numel() const;
void zero_grad();
```

## MUST NOT CONTAIN

* math operations
* backward traversal logic

---

# âœ… tensor.cpp

## Purpose

Implements tensor storage utilities.

## MUST IMPLEMENT

```
Tensor constructor â†’ allocate data + grad
int numel()
void zero_grad()
reshape helper
indexing helper (flat indexing)
```

## Includes

```
#include "tensor.hpp"
```

---

# ğŸ§  autograd.hpp

## Purpose

Declares the gradient engine.

## MUST DECLARE

```
void backward(Tensor& loss);
void topo_sort(Tensor* node, vector<Tensor*>& graph);
```

No implementation here.

---

# ğŸ§  autograd.cpp

## Purpose

Executes backward propagation through computation graph.

## MUST IMPLEMENT

### topo_sort

* DFS through `parents`
* Build ordered graph list

### backward

```
loss.grad = 1
create topo order
iterate reversed order
if backward_fn exists â†’ call it
```

## Includes

```
tensor.hpp
autograd.hpp
```

---

# âš™ï¸ ops.hpp

## Purpose

Declare all tensor operations.

## MUST DECLARE

### PHASE A â€” Core Ops

```
Tensor add(const Tensor&, const Tensor&);
Tensor relu(const Tensor&);
```

### PHASE B â€” Linear Algebra

```
Tensor matmul(const Tensor&, const Tensor&);
Tensor flatten(const Tensor&);
Tensor reshape(const Tensor&, vector<int>);
```

### PHASE C â€” CNN Ops

```
Tensor conv2d(...);
Tensor maxpool(...);
```

No implementations here.

---

# âš™ï¸ ops.cpp

## Purpose

Implements all mathematical operations + autograd behavior.

Every op MUST:

```
create output Tensor
assign parents
define backward_fn lambda
```

---

## PHASE A â€” CORE

Implement:

```
add forward
add backward_fn

relu forward
relu backward_fn
```

---

## PHASE B â€” LINEAR

Implement:

```
matmul forward/backward
flatten
reshape
```

---

## PHASE C â€” CNN (NAIVE)

Implement:

```
conv2d forward/backward
maxpool forward/backward
```

âš ï¸ Use simple loops. No optimizations required.

---

# ğŸ§© nn.hpp

## Purpose

High-level neural network abstraction.

## MUST DEFINE

### Base Class

```
class Module {
public:
    virtual Tensor forward(Tensor x)=0;
};
```

### Layers

```
class Linear;
class ReLU;
class Sequential;
class Conv2D;
class MaxPool;
```

### Training Components

```
class SGD;
Tensor cross_entropy(Tensor logits, Tensor targets);
```

### Metrics

```
size_t count_parameters(Module&);
size_t compute_flops(Module&);
```

---

# ğŸ§© nn.cpp

## Purpose

Implements neural network layers using ops.

---

## PHASE A â€” BASIC NN

Implement:

```
Linear::forward â†’ matmul + add
ReLU::forward â†’ relu
Sequential::forward â†’ sequential execution
```

---

## PHASE B â€” CNN WRAPPERS

Implement:

```
Conv2D::forward â†’ call conv2d op
MaxPool::forward â†’ call maxpool op
```

Do NOT write math here.

---

## PHASE C â€” TRAINING LOGIC

Implement:

```
cross_entropy forward/backward
SGD optimizer step()
```

---

## PHASE D â€” METRICS

Implement:

```
count_parameters(Module&)
compute_flops(Module&)
```

---

# ğŸ”— bindings.cpp

## Purpose

Expose C++ API to Python using pybind11.

---

## FIRST VERSION EXPORTS

```
Tensor
Linear
Sequential
backward
```

---

## FINAL VERSION EXPORTS

```
Conv2D
MaxPool
SGD
cross_entropy
count_parameters
compute_flops
```

## Includes

```
tensor.hpp
nn.hpp
autograd.hpp
```

---

# ğŸ PYTHON IMPLEMENTATION

---

# ğŸ framework.py

## Purpose

Thin wrapper around compiled module.

## MUST CONTAIN

```
import deepframework_cpp
```

Optional aliases allowed.

No training logic here.

---

# ğŸ dataset.py

## Purpose

Load and preprocess images.

## MUST IMPLEMENT

```
load_dataset(folder_path)
infer_labels()
resize_to_32x32()
to_tensor()
batch_loader()
measure_loading_time()
```

Responsibilities:

* read images
* assign labels
* batching
* timing dataset loading

---

# ğŸ§  model.py

## Purpose

Define CNN architecture.

## MUST USE

```
Conv2D
ReLU
MaxPool
Flatten
Linear
```

Only define model structure.
No training loop.

---

# ğŸš€ train.py

## Purpose

Training pipeline.

## MUST PERFORM

```
load dataset
build model
print parameter count
print FLOPs
create SGD optimizer
training loop:
    forward
    loss
    backward
    optimizer.step()
save weights
```

---

# ğŸ“Š evaluate.py

## Purpose

Evaluation script.

## MUST PERFORM

```
load weights
forward pass
compute accuracy
print metrics
```

Script must run without modifying code.

---

# ğŸ”— STRICT DEPENDENCY FLOW

```
tensor â†’ autograd â†’ ops â†’ nn â†’ bindings â†’ python
```

Never reverse this order.

---

# ğŸ‘¥ TEAM OWNERSHIP

## ğŸ‘¤ Engineer A â€” Core Engine

Creates:

```
tensor.hpp
tensor.cpp
autograd.hpp
autograd.cpp
ops.hpp
ops.cpp
```

---

## ğŸ‘¤ Engineer B â€” Neural Network System

Creates:

```
nn.hpp
nn.cpp
```

---

## ğŸ‘¤ Engineer C â€” Python Integration

Creates:

```
bindings.cpp
framework.py
dataset.py
model.py
train.py
evaluate.py
```

---

# ğŸ§¨ FINAL IMPLEMENTATION CHECKLIST

## C++

* Tensor container
* Backward engine
* add / relu / matmul ops
* conv2d / maxpool ops
* Module abstraction
* Linear / Sequential layers
* Conv2D / MaxPool layers
* cross_entropy loss
* SGD optimizer
* Metrics
* Python bindings

## Python

* Framework wrapper
* Dataset loader
* CNN model definition
* Training pipeline
* Evaluation script

---

# ğŸ¯ FINAL GOAL

After implementing all files, the framework must support:

```
Custom Tensor Autograd
CNN Forward + Backward
Python Training Interface
Metrics Reporting
```

Follow this README strictly to prevent circular dependencies and architectural issues.

